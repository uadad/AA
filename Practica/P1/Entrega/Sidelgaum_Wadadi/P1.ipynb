{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae71e392-6857-495b-8f0b-b11127f190b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as panda\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# leemos los datos del fichero\n",
    "datos=panda.read_csv(\"regresion_1.csv\",header=None)\n",
    "# extraemos los valores de atributos (habitantes) y los objeticos (beneficio)\n",
    "x=datos.iloc[:,0]\n",
    "y= datos.iloc[:,1]\n",
    "# representar en un grafica los datos\n",
    "plt.scatter(x, y, color='blue', label='Ejemplos de entrenamiento')\n",
    "plt.title(\"Dataset Original\")\n",
    "plt.xlabel(\"habitantes\")\n",
    "plt.ylabel(\"beneficios\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Obtener los coeficientes por Ecuaciones normales\n",
    "\n",
    "# obtener los valores de entrada y salida en formato columna para hacer el calculo de la ecuacion normal \n",
    "X = np.matrix(x).T \n",
    "y = np.matrix(y).T  \n",
    "\n",
    "# añadir una columna de unos al matriz X para el termino independiente (tita_0)\n",
    "X_b = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "tita = np.linalg.inv(X_b.T * X_b) * (X_b.T * y)\n",
    "\n",
    "print(f\"Coeficientes de la regresión: \\nIntercepto (tita_0): {tita[0, 0]:.4f}, Pendiente (tita_1): {tita[1, 0]:.4f}\")\n",
    "#caluclamos perdicciones\n",
    "y_pred = X_b * tita\n",
    "# Dibujar una grafica con los datos y la linea de regresión \n",
    "plt.scatter(np.array(X), np.array(y), color=\"blue\", label=\"Ejemplos de entrenamiento\")\n",
    "plt.plot(np.array(X), np.array(y_pred), color=\"red\", label=\"Linea de Regresión\")\n",
    "plt.title(\"Ecuaciones Normales (Matrices)\")\n",
    "plt.xlabel(\"habitantes\")\n",
    "plt.ylabel(\"beneficios\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# Calcular el error cuadrático total, medio y por cada entrada\n",
    "squared_errors = np.power(y - y_pred, 2)\n",
    "total_squared_error = squared_errors.sum()\n",
    "mean_squared_error = squared_errors.mean()\n",
    "print(f\"\\nError cuadrático total: {total_squared_error:.4f}\")\n",
    "print(f\"Error cuadrático medio: {mean_squared_error:.4f}\")\n",
    "print(f\"Errores cuadrados por entrada: {np.array(squared_errors).ravel()}\")\n",
    "\n",
    "# funcion para poder dibujar el error que llamaremos en los sigunetes algoritmos\n",
    "def pintar_error(conj_errores):\n",
    " print(f\" Error Cuadratico Medio :{min(conj_errores):.4f} msg\") \n",
    " plt.plot(range(len(conj_errores)),conj_errores,'green',label='Grafica del error')\n",
    " plt.title(\"Evolucion Error(msg)\")\n",
    " plt.xlabel(\"iteracion\")\n",
    " plt.ylabel(\"error\")\n",
    " plt.legend()\n",
    " plt.grid(True)\n",
    " plt.show()\n",
    "\n",
    "# Calculo de los coeficientes mediante Descenso por gradiene\n",
    "x=datos.iloc[:,0]\n",
    "y= datos.iloc[:,1]\n",
    "\n",
    "alfa = 0.0001 # inicializamos la tasa de aprendizaje \n",
    "conj=[123456789,234567890,345678900,456789000,567890000] # conjuntos que contiene las semillas que usaremos\n",
    "resultados_error = []\n",
    "resultados_tita_0 = []\n",
    "resultados_tita_1 = []\n",
    "for k in conj: \n",
    " # diferentes semillas\n",
    " random.seed(k)\n",
    " # incializamos las titas aa valores aleatorios entre 0 y 1\n",
    " tita_0 = random.random() \n",
    " tita_1 = random.random() \n",
    " conj_errores = []   \n",
    " conj_tita_0 = []\n",
    " conj_tita_1 = [] \n",
    " for i in range(3000): # numero de iteraciones para el criterio de parada (30000 iteraciones hasta que converga) \n",
    "   sum_0, sum_1,error  = 0, 0, 0   \n",
    "   for j in range(len(x)): # calculamos el sumatorio del gradiente y lo multiplicamos por el alpha\n",
    "     h_x = tita_0 + tita_1 * x.iloc[j]\n",
    "     sum_0 += (h_x - y.iloc[j])\n",
    "     sum_1 += ((h_x - y.iloc[j]) * x.iloc[j])\n",
    "     error += (h_x - y.iloc[j]) ** 2     \n",
    "   # modificamos los coeficientes restando el valor obtenido para que decrececa las titas en sentido contrario\n",
    "   tita_0 -= alfa * sum_0 \n",
    "   tita_1 -= alfa * sum_1\n",
    "   # añadimos los datos(valores de los coeficientes y el error) a los conjuntos \n",
    "   conj_errores.append(0.5 * error / len(x)) \n",
    "   conj_tita_0.append(tita_0)\n",
    "   conj_tita_1.append(tita_1)\n",
    " # calculamos el valor que minimiza el error para obtener los coeficientes que le corresponden\n",
    " mini = min(conj_errores)\n",
    " ind = conj_errores.index(mini)\n",
    " resultados_error.append(mini)\n",
    " resultados_tita_0.append(conj_tita_0[ind])\n",
    " resultados_tita_1.append(conj_tita_1[ind])\n",
    "# obtenemos los valores de los coeficientes que minimizan el error de los 5 valores correpondientes a las distinats semillas\n",
    "ind_2 = resultados_error.index(min(resultados_error))  \n",
    "coeficiente_0 = resultados_tita_0[ind_2]\n",
    "coeficiente_1 = resultados_tita_1[ind_2]\n",
    "pintar_error(conj_errores)\n",
    "print(f\"Coeficientes de la regresión: \\nIntercepto (tita_0): {coeficiente_0:.4f}, Pendiente (tita_1): {coeficiente_1:.4f}\")\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(x, y, color='blue', label='Ejemplos de entrenamiento')\n",
    "x_linea = np.linspace(min(x), max(x), 1000)\n",
    "y_linea = coeficiente_0 + coeficiente_1 * x_linea\n",
    "plt.plot(x_linea, y_linea, color='red', label='Linea de regresion')\n",
    "\n",
    "plt.title('Descenso por Gradiente')\n",
    "plt.xlabel('habitantes')\n",
    "plt.ylabel('beneficios')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Descenso por gradiente estocastico\n",
    "alfa = 0.0001 # inicializamos la tasa de aprendizaje \n",
    "conj=[123456789,234567890,345678900,456789000,567890000] # conjuntos que contiene las semillas que usaremos\n",
    "resultados_error = []\n",
    "resultados_tita_0 = []\n",
    "resultados_tita_1 = []\n",
    "batch = 50 # numero de valores que usamos para el calculo de la formula del gradiente estocastico \n",
    "for k in conj:\n",
    " # diferentes semillas\n",
    " random.seed(k)\n",
    " # incializamos las titas aa valores aleatorios entre 0 y 1\n",
    " tita_0 = random.random() \n",
    " tita_1 = random.random() \n",
    " conj_errores = []   \n",
    " conj_tita_0 = []\n",
    " conj_tita_1 = []\n",
    " error = 0\n",
    " for i in range(5000): # numero de iteraciones para el criterio de parada (hasta que converga) al ser \n",
    "                       # aleatorio aumentamos el numero de iteraciones para ver si se puede tener mejor solucion\n",
    "   sum_0, sum_1, error = 0, 0,0\n",
    "   conj_batch=[]\n",
    "   for h in range(batch): # obtener 10 datos de los ejemplos para el caluclo del gradiente estocastico\n",
    "     num = random.randint(0,len(x)-1)\n",
    "     conj_batch.append((x.iloc[num], y.iloc[num]))\n",
    "   for j in conj_batch: # caluclar el sumatorio de gradiente estocastico del conjunto obtenido antes\n",
    "     h_x = tita_0 + tita_1 * j[0]\n",
    "     sum_0 += (h_x - j[1])\n",
    "     sum_1 += ((h_x - j[1]) * j[0])\n",
    "     error += (h_x - j[1]) ** 2\n",
    "   # modificamos los coeficientes restando el valor obtenido para que decrececa las titas en sentido contrario\n",
    "   tita_0 -= alfa * sum_0\n",
    "   tita_1 -= alfa * sum_1\n",
    "  # añadimos los datos(valores de los coeficientes y el error) a los conjuntos \n",
    "   conj_errores.append(0.5 * error/batch)\n",
    "   conj_tita_0.append(tita_0)\n",
    "   conj_tita_1.append(tita_1)\n",
    " # calculamos el valor que minimiza el error para obtener los coeficientes que le corresponden\n",
    " mini = min(conj_errores)\n",
    " ind = conj_errores.index(mini)\n",
    " resultados_error.append(mini)\n",
    " resultados_tita_0.append(conj_tita_0[ind])\n",
    " resultados_tita_1.append(conj_tita_1[ind])\n",
    "# obtenemos los valores de los coeficientes que minimizan el error de los 5 valores correpondientes a las distinats semillas\n",
    "ind_2 = resultados_error.index(min(resultados_error))\n",
    "coeficiente_0 = resultados_tita_0[ind_2]\n",
    "coeficiente_1 = resultados_tita_1[ind_2]\n",
    "print(f\"Coeficientes de la regresión: \\nIntercepto (tita_0): {coeficiente_0:.4f}, Pendiente (tita_1): {coeficiente_1:.4f}\")\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(x, y, color='blue', label='Ejemplos de entrenamiento')\n",
    "x_linea = np.linspace(min(x), max(x), 1000)\n",
    "y_linea = coeficiente_0 + coeficiente_1 * x_linea\n",
    "plt.plot(x_linea, y_linea, color='red', label='Linea de regresion')\n",
    "plt.title('Descenso por Gradiente Estocastico')\n",
    "plt.xlabel('habitantes')\n",
    "plt.ylabel('beneficios')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "def calcular_Predicado(coeficiente1, coeficiente2, valor_entrada):\n",
    "  return coeficiente1 + coeficiente2 * valor_entrada\n",
    "\n",
    "valor = 7.5\n",
    "pred = calcular_Predicado(tita[0,0],tita[1,0],valor)\n",
    "\n",
    "#print(f\"El beneficio que corresponde a una poblacion de {valor} personas es : {calcular_Predicado(tita[0,0],tita[1,0],valor):.4f} €  \")\n",
    "#print(f\"El beneficio que corresponde a una poblacion de {valor} personas es : {calcular_Predicado(coeficiente_0,coeficiente_1,valor):.4f} €  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75de233-9777-4a3e-8100-1d80b6d2ca23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11699c3d-5fb3-4a1a-a81c-0a3f176e105c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431e4765-3dd2-4ea9-afb9-f32ceec16baf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
